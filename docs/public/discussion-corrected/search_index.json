[
["index.html", "Experiment #1: centralized vs decentralized search 1 Introduction", " Experiment #1: centralized vs decentralized search Kabir Veitas (kabir@singularitynet.io) 2018-10-20 1 Introduction Here I discuss and provide broader interpretation of results documented in the Electronic Laboratory Notebook: Set 1: experiment-1-run-1-corrected experiment-1-run-2-corrected experiment-1-run-3-corrected Set 2: experiment-1-run-4 experiment-1-run-5 Set 3: experiment-1-run-6 experiment-1-run-7 All data used in this document in *.Rdata format is available from github repo, for earch experiment separately in directory https://github.com/singnet/offernet/blob/master/docs/{experiment name as above}/R_data/summary_of_all_experiments.Rdata. Source code of all analysis documents is available from singnet/offernet/docs directory repository. Section Decentralized computing in OfferNet(s) discusses the concept of open-ended decentralized computing with relationship to Offer Networks. Section Experiment #1:comparison of decentralized and centralized search provides analysis and interpretation of data obtained from running computer simulations. All the code used by simulations and documentation (including this page) can be accessed and downloaded from singnet/offernet GitHub repository. "],
["decentralized-computing-in-offernets.html", "2 Decentralized computing in OfferNet(s) 2.1 Open-ended decentralized computing 2.2 Offer Networks 2.3 Footnotes", " 2 Decentralized computing in OfferNet(s) We consider a simplified framework of decentralized barter exchange between independent and heterogeneous agents – Offer Networks – for demonstrating main properties of what we call open-ended decentralized computing which is a paradigm for researching and testing the possibility of computational emergence of open-ended intelligence (D. Weinbaum (Weaver) and Veitas 2017b; Weinbaum (Weaver) 2018) by performing computer simulations. We first shortly introduce main concepts of open-ended decentralized computing and then show how they are being mapped to the Offer Networks architecture. Finally, results of preliminary experiments are presented and discussed. We discuss the possibilities of opportunities of extension of Offer Networks architecture as we go. 2.1 Open-ended decentralized computing Open-ended intelligence is an abstraction of the process of human cognitive development, extended to general agents and systems. Three facets of it are (1) the philosophical concept of individuation, (2) sense-making and (3) the individuation (via progressive determination) of general cognitive agents. Open-ended intelligence can be framed in terms of a decentralized, self-organising scalable network of interacting agents. A critical aspect of individuation of intelligence is disparity resolution and coordination among independent and heterogeneous agents which a priori hold distinct value systems. See (D. Weinbaum (Weaver) and Veitas 2017b) for the introduction of the concept in the context of Artificial General Intelligence and (Weinbaum (Weaver) 2018) for in-depth metaphysical treatment of the framework. 2.1.1 Basic principles Main aspects of open-ended decentralized computing, derived from the open-ended intelligence philosophy are: Network of heterogenous agents. First, since open-ended intelligence is framed in terms of decentralized network of interacting agents, it determines the usage of an Actor model of computation (Hewitt, Bishop, and Steiger 1973; Agha and Hewitt 1985; Hewitt 2013, 1976) as a computational framework. Actor model features communication medium where many independent and heterogeneous agents interact via message passing. Even though each individual actor in the framework can represent a deterministic computation (algorithm), a network of actors in its general form is non-deterministic – probabilistic and deterministic behaviours can be represented using it, but the general formalism itself is not constrained in any way by them. Stigmergic coordination. Second, the possibility of progressive determination of emergent structures in a network (as assemblages of actors) is implemented by applying the universal mechanism of stigmergic coordination (Heylighen 2016). Stigmergy is a mechanism of indirect coordination between actions of otherwise independent actors via the shared medium or context where individual actions shape the medium and medium influences further individual actions in recursive way. Thus generalized concept of stigmergy is equivalent to the concept of progressive determination, which is the actual mechanism of individuation (D. Weinbaum (Weaver) and Veitas 2017b; Veitas and Weinbaum 2017). In Open-ended decentralized computing, the shared medium allowing for stigmergic coordination and progressive determination is implemented as a graph (concretely – property graph (Robinson, Weber, and Eifrem 2015; Rodriguez and Neubauer 2010)) which can be read from and written to by each actor in a concurrent and asynchronous manner. Often used notion of stigmergy to denote indirect communication in colonies of eusocial insects via pheromone traces is but a special case of the general concept. Decentralization. Third, open-ended decentralized computing meticulously follows the principle of open-ended intelligence in that it is not constrained by any a priori goals. We conceive a framework which is not constrained by pre-defined structures and general goals, yet where they can emerge from the interaction among elements or indirectly influenced by limiting behavioural repertoire of agents participating in the network. Implementation wise, we strictly enforce the principle of decentralization – positing that there could exist no actor or agent a priori having complete and full knowledge of the network (i.e. underlying graph structure) or control over it1. Such agent in principle could emerge from indirect or direct coordination happening in the network, but it cannot obtain such “privileged” status before interaction begins (hence – no a priori). The principle of decentralization, apart from conceptual importance, influences many software design and rather low level decisions on coding level – e.g. prohibition of global and shared data structures apart the network itself 2. 2.1.2 Directions of simulation modelling research Open-ended decentralized computing is a research direction and a practically applicable paradigm. The chosen research method for advancing it is simulation modelling which encompasses two aspects: (1) designing and implementing a software architecture based on aforementioned principles having practical importance and (2) running computational experiments on it. OfferNet(s) software architecture 3 is such a framework. Purposes of simulation modelling research reflect the double nature of Offer Networks – as an implementation of open-ended decentralized computing framework and a pragmatic application of it for solving well-defined (or definable) problems. We first start with delineation of the former and then go on to describe the latter. From the broadest conceptual perspective we want to operationalize the principle of progressive determination in a working software architecture. Such an architecture would demonstrate practical aspects of open-ended intelligence philosophy and pave a way for its application in many contexts, including, but not limited to the quest for the Artificial General Intelligence. In summary, the software architecture is meant for researching: The very possibility of implementing the mechanism of progressive determination in the computational medium; Identifying and demonstrating contexts where application of decentralized mechanism of progressive determination makes more sense from practical perspective than conventional centralized algorithms; Demonstrate bottom-up emergence of higher order structures in the network having detectable, yet not a priori defined functionality in the context of the network; such structures take form of ad-hoc assemblages of lower level actors. 2.2 Offer Networks Offer Networks is a concept of an alternative economy where Agents (humans, AIs and more or less simple programs and intelligences) find, negotiate and execute locally and globally beneficial series of hybrid exchanges (monetary and non-monetary) of tangible and/or intangible goods. It was first proposed by (Ben Goertzel 2015b) in the context of post-money economy and further developed by (Heylighen 2017) in the context of Global Brain research. It is a research project aiming at conceiving, conceptualizing and proof-of-concept implementation of alternative economy for the age of decentralized and autonomous AI technologies. Notwithstanding above, on a higher conceptual level, Offer Networks is a practical context enabling implementation and testing of the mechanism of progressive determination and generalized stigmergic cooperation in a computational medium. Offer Networks model is conceived as a special case of certain computational aspects of the descriptive model of the individuation of cognition (D. Weinbaum (Weaver) and Veitas 2017a, 2017b). As such it adheres to the main conceptual principles of the open-ended intelligence and decentralized computing, but does not attempt to fully cover them. Relation between conceptual models of individuation of cognition and Offer Networks is well conveyed via graphical representations of both: Figure 2.1: Conceptual model of individuation of cognition in terms of relationship among scales, populations and boundaries. The chosen scale of analysis is \\(S\\). \\(S+1\\) is the higher scale while \\(S-1\\) is the lower scale. \\(P_{s}\\) denotes a population of agents at scale \\(S\\). Solid circles denote the agents of population \\(P\\) at any scale. Dashed lined circles denote super-agents at any scale: e.g. – \\(A_{s}\\) at the center of the figure, denotes a super-agent that emerges from the interactions of agents in \\(P_{s}\\). Super-agents at scale \\(S\\) are the agents of the population \\(P_{s+1}\\). The \\(i-th\\) super-agent at scale \\(S\\) is denoted \\(A_{s}^{i}\\), the superscript is omitted if unneeded. Also, the subscript \\(S\\) is omitted from \\(A\\) or \\(P\\) in the text if it is redundant (adapted from (D. Weinbaum (Weaver) and Veitas 2017a)). Figure 2.2: Conceptual architecture of OfferNet as a self-organizing network of interacting agents Most importantly, both models describe interactions between heterogeneous agents that give rise to higher order behaviours of assemblages of agents. The main difference is that the model of individuation of cognition is very abstract, while the OfferNet model features specific types of agents and their relations as required by the domain. We will not go into details of the mapping of the models, but rather explain in more detail components of Offer Networks model. Let us first start with the data structure and then explain distributed processes performed on it by agents. 2.2.1 Data structure The data structure of OfferNet(s) is a property graph, composed from nodes of type \\([agent, work, item]\\) and edges of type \\([knows, owns, demands, offers, similarity]\\): Figure 2.3: OfferNet(s) graph structure: note that the initial structure does not contain similarity links, which appear in the graph only after running similarity search processes. Agents form a type of social network by relating to each other via knows links. The only hard constraint for the network of agents is that it should be a connected graph (i.e. there is a path between each vertices). Nevertheless, the effectiveness of processes running on this network very much depends on how they are connected – i.e. the topology of \\(agent \\xrightarrow{\\text{knows}} agent\\) subnetwork (see discusion). Agents also relate to one or more \\(work\\) via \\(owns\\) links – representing situations when an agent publishes what it ‘wishes’ to exchange something in the network; A work represents a specialized ‘process of exchange’ that an agent is willing to execute if any interested parties exist in the network. As every process, \\(work\\) has inputs (\\(demands\\)) and outputs (\\(offers\\)). In the current simplified model a \\(work\\) features only one \\(demand\\) and one \\(offer\\). In principle arbitrary complex works can be represented – e.g. featuring more than one input (energy, computational resources or a monetary payment) or output. This aspect is particularly interesting with respect to potential integration of OfferNet(s) framework to SingularityNET – considering that a \\(work\\) can involve ‘exchange’ of data output (e.g. text description of image) for data input (e.g. image to be described) and certain amount of AGI tokens. Note that in this case, a \\(work\\) would not be a ‘process of exchange’ but rather a ‘process of text summarization’ which nevertheless can be perfectly well represented within the same framework. Finally, in OfferNet(s), a \\(work\\) connects to \\(items\\) of exchange via \\(demands\\) or \\(offers\\) link. An item is an actual item of exchange. In OfferNet(s) this is limited to actual physical or non-physical items where \\(demands\\) link means that it is an input and \\(offers\\) – output. In a more generalized model an item could be a representation of any input or output (data, token, energy units, etc.); 2.2.2 Research questions The ambition of Offer Networks (as an alternative economy) is to conceive, implement and test mechanisms of search, matching and execution of exchange of goods and services in a decentralized system both in terms of information and control1. Note, that such definition of decentralization does not enforce homogeneity of agents – i.e. agents can wildly differ in their knowledge of the network and exercisable control, yet still be far from omniscient and omnipotent at the scale of the whole network. Actually, such dynamic heterogeneity is a desirable property leading to beneficial social network dynamics (Veitas and Weinbaum 2017). Also note that this definition somewhat differs from the original formulation of the goal of Offer Networks as search and execute globally optimal set of exchanges that would maximally satisfy outstanding offers/demands by maximum number of agents (Ben Goertzel 2015a) by relaxing optimality requirement. We are working towards this goal by asking (and answering) concrete research questions which allow for clear formulation and testing of scientific hypotheses – a process leading to incremental building of the system. The current research horizon encompasses following questions: What are parameters that determine advantages and disadvantages of decentralized and centralized search algorithms in different contexts? In principle, Offer Networks goals can be achieved by either centralized (global) or decentralized (local) processes running on the same data structure as defined earlier. In practice, however, the feasibility of any of the approaches is largely determined by concrete circumstances and context-specific aspects. For example, centralized algorithms can optimize results at the cost of combinatorial explosion of computational complexity needed to carry them out, while decentralized algorithms may achieve sub-optimal, but still “good enough” results faster at the cost of giving up control on the whole data structure. The very term “good enough” implies context dependency. In order to provide at least some insights to this question we set-up centralized and decentralized search processes in Offer Networks and test them with different parameters (see design and discussion of the experiment on comparing decentralized and centralized search). Can we conceive decentralized processes which make themselves more efficient by utilizing results of ‘traces’ left by preceding processes? This research question addresses the feasibility of implementing stigmergic coordination principle of the open-ended decentralized computing as formulated above in different contexts. Such processes, interacting among each other in a decentralized way, would implement learning ability of the network. Similar to the first research question, the learning ability of the network can be implemented in a centralized or decentralized manner. The design of the experiment needed to answer this research question is informed by the insights from results of the first experiment – see its further steps. How can we utilize learning network of heterogeneous processes for implementing a decentralized exchange of goods and services? Answering this research question requires additional conceptual work is needed as well integration of the answers and insights from research question 1 and 2 in an Offer Networks system. The actual design of experiments required for answering this research question will be approached after obtaining and interpreting results of the experiment based on the second research question. 2.2.3 Processes Following basic principles of open-ended decentralized computing, Offer Networks is implemented as an ecosystem of decentralized processes interacting via the stigmergic medium. Note, that on the general level these principles are in line with the conceptual model of Cognitive Synergy – “a dynamic in which multiple cognitive processes, cooperating to control the same cognitive system, assist each other in overcoming bottlenecks encountered during their internal processing”. The model of cognitive synergy is used explicitly in the design of OpenCog cognitive architecture (Goertzel 2017). The list of decentralized processes is open-ended – any process can be added by an agent participating in the network. Processes required for basic functionality of OfferNet(s) are: Similarity search; Find cycles of changeable items; Execute exchange cycles; Find and connect items of exchange via similarity links; These processes are described in detail below. Additionally, every experiment designed to answer a specific research question can feature specialized processes for that purpose – discussed under description of an experiment. Finally, OfferNet(s) architecture can be broadly extended by adding different processes and their interactions – discussed in further steps following Experiment 1. 2.2.3.1 Process#1: Similarity search Similarity search process searches similar items in the network and connects them with similarity links. The ability to measure similarity of items is based on a uniform representation and description of item value. Similarity measure also can take different forms. Note, that all these measures are global parameters of simulation framework and different forms of it can be ‘plugged’ and ‘unplugged’ from the system. Furthermore, in a decentralized system, nothing prevents agents to agree on the usage of different similarity measures within the same framework. For the visual representation of how similarity search mutates the graph, see figures below. Figure 2.4: Graph mutations due to similarity search process illustrated on the toy graph. Figure 2.5: Visualization of larger graph mutations due to similarity search process. Initially items are related only indirectly via agents that have posted them into a network. The goal of similarity search process is to connect items directly and by this keep the data structure of OfferNet(s) in a form that would make finding cycles possible and efficient. In the current version of OfferNet(s) software framework4, item values are represented as real numbers in the range \\([0,1]\\). Similarity between two items is then calculated using the formula \\(Sim = 1 - abs(value_{i1} - value_{i2})\\) which also results in the real number of range \\([0,1]\\). The closer the number to one, the more similar items are. Algorithmically, similarity search is implemented in two flavours – centralized and decentralized. Comparison of performance of them forms the basis of Experiment 1 (see description further). 2.2.3.1.1 Centralized similarity search Centralized similarity search simply fetches all items in the network, compares each item value with every other and creates similarity link between them if the similarity value exceeds a parameter called similarityConnectThreshold. This parameter regulates the density of connectivity between items on the one hand and the ability for agents to exchange “fuzzy” similar items on the other. Centralized similarity search requires a full scan of the graph in order to collect data on all items demanded or offered by agents at certain moment in time, combining this data into a single data structure and then processing it in a centralized (but possibly distributed) manner. Currently the process is implemented by OfferNet.connectAllSimilarCentralized() routine. 2.2.3.1.2 Decentralized similarity search Decentralized similarity search, contrary to the centralized flavour, works only on behalf of an agent that initiates the search and does not require fetching all item values from the network. On the other hand, a decentralized process requires concurrent and asynchronous execution on behalf of each agent. It operates as a spreading activation which starts with the items of the initiating agent and checks similarity of them with those of agent’s neighbours. Decentralized similarity search takes similarityConnectThreshold and maxDistance parameters. The former serves the same way as in centralized search, while the latter determines how far into the neighbourhood of an agent spreading activation process traverses. Currently the process is implemented by Agent.searchAndConnect(...) routine which roughly follows this logic: # parameters: # -- me: the agent that initiates traversal # -- maxDistance: number of hops in traversal; # -- similarityConnectThreshold: only items with this and higher similarity are connected; myItems &lt;- me.getAllDemands() + me.getAllOffers(); discoveredItems &lt;- emptyList(); distance = 0; function getItemsOfNeighbours(agents): for each agent in agents do: discoveredItems &lt;- discoveredItems + agent.getAllDemands() + agent.getAllOffers(); neighbours &lt;- agent.knowsAgents(); getItemsOfNeighbours(neighbours); distance = distance +1; if distance = maxDistance do: break from cycle; getItemsOfNeighbours(me) for each discoveredItem in discoveredItems do: for each myItem in myItems do: similarityValue = calculateSimilarity(discoveredItem, myItem) if similarityValue &gt;= similarityConnectThershold do: createLink(from: myItem, to: disoveredItem, type: similarity, value: similarityValue) 2.2.3.2 Process#2: Find cycles of changeable items In graph theory, a cycle is a collection of vertices and edges among them where each vertex present in the collection is reachable from itself via the edges present in the collection. Cycle search is the process that finds such data structures in a messy and unstructured graph. A cycle discovered in OfferNet(s) (see figure 2.6 below) represents a match of demands and offers of at least two agents participating in the marketplace and the possibility of actual exchange of their items. More formally, it is a subgraph of the OfferNet(s), where agents “know” other agents’ preferences with respect to the discovered match and can agree to execute (or not) the exchange cycle (see process #3). Figure 2.6: Cycles discovered in the OfferNet(s) graph by cycle search processes; note how it relates to the conceptual architecture of Offer Networks (figure 2.2) For the purposes of Experiment #1, cycle search process is implemented by Agent.cycleSearch(...) routine, which takes a similaritySearchThreshold as an argument. This argument determines, how much ‘fuzzyness’ an agent is willing to accept when considering options to exchange items with other agents. E.g. if \\(similaritySearchThreshold = 1\\), only strictly equivalent items will be considered for exchange, otherwise – certain amount of variability of items can still be considered acceptable. Note, that in a distributed system each agent participating in the exchange may have a different similaritySearchThreshold parameter (signifying an individual preference). Cycles are temporary structures – they get dissolved when executed (see process #3). Actually the system can be considered more beneficial the more cycles are discovered, executed and dissolved in OfferNet(s) per unit of time. Another important aspect, emphasized in figure 2.6 is that while cycles are of dynamic nature – emerging and dissolving during operation of the OfferNet(s) graph – they are basic elements of conceptual architecture as a self-organizing network of interacting agents (see figure 2.2). 2.2.3.3 Process#3: Execute exchange cycles Finding cycles only increases the probability that certain items will be exchanged but is far from guaranteeing it. While in a centralized system an execution of the discovered cycle is straightforward, it is not so in a decentralized case. First, more than one cycle can emerge involving overlaps of agents and items in which case some sort of consensus has to be reached via negotiations on which cycle will be executed and which will be neglected. Second, when a cycle involves fuzzy matches, the willingness of agents to exchange items that are not strictly similar have be confirmed. Third, in real cases the presence of insufficient information in the network due to incomplete preferences (Mandler 2005) should be considered – implying the necessity of a negotiation round between agents involved in the cycle before its execution. Furthermore, exchanges in distributed systems cannot rely on a single provider of trust and therefore must use a distributed trust model (Abdul-Rahman and Hailes 1997) including, but not necessarily limited to the blockchain technology (Swan 2015) and a reputation system or systems (Kolonin et al. 2018). 2.2.3.4 Process#4: Search and connect similar items of exchange via similarity links Success and computational complexity of finding matches in the graph is mostly due to the similarity search process rather than cycle search (see discussion and [future steps] #1](#insights-and-future-steps) of Experiment #1). Similarity search via the \\(agent \\xrightarrow{\\text{knows}} agent\\) subnetwork is relatively costly because it requires to check many agents in the neighbourhood graph in order to find a similar item. Furthermore, in a decentralized scenario and depending on the topology of the graph the search may not succeed even the cycle exists (see sensitivity to graph topology aspect), It may be much more efficient to traverse similarity links between items directly (i.e. utilizing graph topologies on the right rather than left of figures 2.4 and 2.5). The process would adhere to the following logic: # parameters: # -- me: the agent that initiates traversal # -- maxDistance: number of hops in traversal; # -- similarityConnectThreshold: only items with this and higher similarity are connected; # function getItemsOfNeighbours(agent) is defined in pseudocode for Process #1; myItems &lt;- me.getAllDemands() + me.getAllOffers(); distance = 0; function searchAndConnectViaSimilarityLinks(myItem, otherItem): similarityValue &lt;- calculateSimilarity(item, otherItem) if similarityValue &gt;= similarityConnectThreshold do: createLink(from: myItem, to: iterItem, type: similarity, value: similarityValue) if distance &gt; maxDistance do: break from the loop; else: nextMostSimilarItem &lt;- item.getSimilarItems(max(similarityValue)) searchAndConnectViaSimilarityLinks(myItem, nextMostSimilarItem) for each myItem in myItems do: distance = distance + 1; if distance = maxDistance do: break from cycle; if item.hasSimilarityLinks() do: mostSimilarItem &lt;- item.getSimilarItems(max(similarityValue)) searchAndConnectViaSimilarityLinks(myItem, mostSimilarItem) else: itemsOfNeighbours &lt;- getItemsOfNeighbours(me) for each discoveredItem in itemsOfNeighbours do: searchAndConnectViaSimilarityLinks(myItem, discoveredItem) As specified, this process traverses \\(item \\xrightarrow{\\text{similarity}} item\\) subgraph directly if it exist, otherwise, reverts back to traversing \\(agent \\xrightarrow{\\text{knows}} agent\\) subnetwork as defined by Process #1. In OfferNet(s) framework many such processes operate concurrently on behalf of different agents, yet each process changes overall graph structure that is traversed by other processes, therefore they together lead to the desired dynamics of progressive determination. Experiment #1, discussed further, implements processes #1 and #2 and compares running times of their centralized and decentralized counterparts. Experiment #2 (not yet carried out) will implement process #4. Process #3 is due for implementation in yet further experiments. 2.3 Footnotes 1: Two types of decentralization can be distinguished – decentralization of information and decentralization of control. Decentralization of information means that no actor in a network can have full knowledge of the whole network (i.e. to know its global state), while decentralization of control – that no single agent can exercise actions that single-handedly determine dynamics of the network. 2: Actually, even the usage of a global structure (graph) for representing the network itself is determined by technical/pragmatic convenience rather than conceptual necessity. 3: https://github.com/singnet/offernet; development is supported by SingularityNET Foundation (https://singularitynet.io/research-initiatives/). 4: singnet/offernet git repository, commit number d784d1c. References "],
["experiment-one.html", "3 Experiment #1: comparison of decentralized and centralized search 3.1 Setup 3.2 Discussion 3.3 Insights and future steps", " 3 Experiment #1: comparison of decentralized and centralized search With this experiment we would like to see how centralized similarity search and cycle search processes perform with comparison to decentralized processes. Preliminary design of the experiment is explained here. 3.1 Setup For that purpose we have implemented Process #1 and Process #2 in centralized and decentralized flavours. In order to compare them we follow these steps: First, we create an Offer Network of predefined size (parameter agentNumber); We experiment with the random graph (where agents are randomly connected with knows links) and a small-world graph, where we know the diameter of the network in advance; Then we artificially create a list of items which, if correctly searched and connected in the OfferNet(s) graph, would form a chain. The length of the predefined chain is set by parameter chainLength; Items from the list are assigned to random agents in the network; Then, we create a special taskAgent owning a work which, when correctly connected to the potential chain inserted into the network by step 2, closes it into a loop forming a cycle (as shown in the figure 2.6). Finally, we run the decentralized and centralized processes on the same graph and log running times of each method. 5.1. Similarity search process connects all similar items with ‘similarity’ links, as explained here; 5.2. Cycle search process is run on behalf of taskAgent and discovers the cycle inserted by step 2 (in case similarity search process correctly connected similar items) – as explained here; Note, that in decentralized case the all times of running the process on behalf of separate agents is aggregated. An experiment is a series of simulations, each of which takes the following parameters: agentNumber: number of agents in the network (apart from taskAgent); similarityConnectThreshold: the minimum similarity value between items connected with explicit link by similarity search process; chainLength: the length of the chain inserted into the network by step 2; similaritySearchThreshold: minimal similarity of items to be considered as eligible for exchange; maxDistance: radius of agent’s neighbour network when searching for similar items; randomWorksNumberMultiplier: the number of random works and items which are assigned to the agents in the network to make cycle search more realistic; Figure 3.1: Distribution of simulation parameters of all analysed experiments ( 1194 simulations in total). Detailed data about all experiments, simulations, their parameters and descriptive analysis of obtained results is provided in Electronic Laboratory Notebook here. In the next two sections we discuss insights of the analysis and further steps based on them. 3.2 Discussion 3.2.1 On decentralized versus centralized computation As many non-trivial (and interesting) questions, the issue of whether centralized or decentralized models are “better” cannot be answered in a univocal manner. The short and not very informative answer to this question would be “it depends” – you can always find cases and examples where one of them works better. Our goal is therefore not to answer ‘yes or no’ but to figure out parameters and circumstances where one or another type of model or algorithm fairs better. Notwithstanding what was said above, decentralized and centralized computation models are not equal in terms of their expressivity. It can be shown that centralized computation is a special case of the decentralized model. For this we have to establish a relation between non-determinism and decentralization. Recall first that any computation or a program can be expressed as a graph of atomic program steps (as nodes) and transitions between them (as links) (Turchin 1986; Pennachin, Goertzel, and Geisweiller 2014). In general any Turing machine can be represented as a graph (Laud 2011): Figure 3.2: Representing Turing machines as graphs (adapted from (Laud 2011)). A centralized system is a system where all transitions between atomic program steps are known and controlled by a central observer – this corresponds to the figure on the left. In a decentralized system, every atomic program has a freedom to choose any possible transition and this choice cannot be a priori known or controlled in any manner by the central observer – this corresponds to the figure on the right. It is easy to see, that left image is the special case of right image, i.e. a non-deterministic / decentralized computation can be reduced to deterministic / centralized by pruning a number of links in the initial graph. Bottom line, is that it makes sense to start designing a computational framework or architecture based on decentralized model but allow for a centralized computation to emerge out of it rather than the other way round. Even if emergence is not considered due high computational costs and overall unpredictability, it still makes sense to use decentralized framework and represent centralized processes ‘manually’. 3.2.2 Sensitivity to graph topology One of the first hypotheses that experiments seem to support is that decentralized and centralized search have very different sensibility to the underlying graph topology. That is, centralized search algorithm is more sensitive to how many agent nodes are in the graph and not on how they are connected, while decentralized search is sensitive to the topology of \\(agent \\xrightarrow{\\text{knows}} agent\\) subgraph. This is because decentralized graph traversals continue only as deep as constrained by maxDistance parameters (which defines the radius of agent network to be searched – see setup) and if the diameter of graph is larger than this radius, then the cycle may not be found (even though it exists in a network). It is of course possible to increase maxDistance parameter to the arbitrary large number, but this also may increase time of search drastically (see graph below). Figure 3.3: Dependence of simulation time on maxDistance parameter. For confirming the sensitivity of algorithms to graph topology we also ran experiments with the same parameters on two different graph topologies – (1) randomly connected and (2) small world with diameter of less than 10. As shown in the diagram below, success rate of finding a cycle in a random graph is often lower than 100% (except when the depth of each traversal is made very large), while working on small world graph with a known diameter and corresponding maxDistance parameter guarantees that the cycle will be found if it exists in the graph even in a decentralized scenario. Figure 3.4: Success rate of finding a cycle in random and small world graphs, depending on maxDistance parameter. Small world graphs were with known diameter of =&lt; 10. Note that with random graphs, maxDistance of 5 and 10 is not enough to traverse the whole graph, while 30 often provides (but does not guarantee!) full coverage. Similarly, maxDistance of 5 is not always enough to traverse small world graph which has larger diameter of around 10. 3.2.3 Decentralized search time increases by the number of edges As provided by the results of experiments with parameters described in table 3.1, decentralized search is marginally faster when there is relatively small number of links in the graph, yet its time complexity quickly increases with the number of similarity edges (see figure 3.5. Dependency is plotted separately for graph with different agentNumber parameters, since number of vertices and number of edges in the graph are not independent variables – agentNumber is the parameter of simulation, but number of edges is an emergent variable. We use the number of similarity edges rather than total number of edges in the graph only for the reason that the number of similarity edges exceeds numbers of other types of edges by a few orders of magnitude and renders them ‘unimportant’. Figure 3.5: Dependence of simulation time on similarity edges in the graph as modulated by similarityConnectThreshold parameter for different agent numbers. Note, that centralized search is somewhat faster than decentralized which has to be investigated further. 3.2.4 Centralized search time increases by the number of vertices Theoretically, centralized search time depends on the number of vertexes in the graph, due to the way centralized search works (see explanation). In practice it is somewhat problematic to estimate, since number number of edges is very much dependent from number of vertices in the graph and it is difficult to isolate the two – especially since number of edges is an emergent variable and not a parameter that can be manually set. Nevertheless, the dependency of simulation times on agentNumbers indicates that time increases with agentNumber. Figure 3.6: Time needed for centralized search roughly increases depending on number of item vertices in the graph: important parameters used for simulations displayed: agentNumbers=[50, 100, 200, 400,500,600,700,800]; Note, that in case of similarityConnectThreshold=0.9, there seems to be an anomaly, causes of which have to be investigated further. 3.3 Insights and future steps Based on the results of experiments and above discussion we can confidently make the following considerations in order to guide further design of the software framework: First, centralized search is not feasible for graphs with large number of nodes. This is not very surprising as it is known that many graph algorithms tend to behave exponentially. Note, that even in case of “moderate” exponential behaviour it is not acceptable for us, since the goal of the Offer Networks architecture (and open-ended decentralized computing framework in general) is to conceive conceptual and computational model able to achieve practical solutions on very large graph data structures – with at least millions of vertices and tens of millions of edges. Smaller graphs, while may be useful for academic research purposes, do not sound interesting enough for devising a computational model of alternative economy. Second, decentralization is sought as a conceptual remedy for intractability of centralized algorithms when run on very large (and dynamic) data structures. As explained above, decentralization comes at a cost of increased aggregated computational complexity and loss of optimality but in principle enables to achieve asymptotic results in practically acceptable time frames. While decentralized search algorithms of OfferNet(s) do not perform very well when running on (1) graph structures with large diameters of \\(agent \\xrightarrow{\\text{knows}} agent\\) subgraphs and (2) graphs with large amount of similarity links between items, decentralization provides possibilities of optimizations, which are not available in centralized computation; Decentralized search can be greatly improved by following optimizations to be explored by Experiment #2: smartly constructing traversals which dynamically prune search tree based on certain criteria, which could be individual for each agent. In OfferNet(s) model such criteria could be the degree of similarity among items; controlling topology of the graph by connecting items of the graph into \\(item \\xrightarrow{\\text{similarity}} item\\) subgraph with bounded diameter and then running search processes #4 along with processes #1. Last, but not least, as demonstrated by running times on graphs with large number of edges (see figure 3.5) large scale simulations are needed to reveal certain non-linear behaviours that cannot be extrapolated from small scale simulations alone (i.e. the observation that time needed for decentralized search exceeds centralized search only when number of edges is roughly above one million). References "],
["references.html", "References", " References "]
]
